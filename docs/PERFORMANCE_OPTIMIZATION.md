# 3ë§Œ RPS ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ

## ğŸ¯ ëª©í‘œ
- **ì²˜ë¦¬ëŸ‰**: 30,000 RPS
- **ì§€ì—°ì‹œê°„**: P99 < 100ms
- **ê°€ìš©ì„±**: 99.9% uptime

## ğŸ“Š ìµœì í™” ì „ëµ

### 1. HPA/PDB ì„¤ì •

#### Gateway API (Entry Point)
```yaml
minReplicas: 10  # í•­ìƒ 10ê°œ ìœ ì§€
maxReplicas: 50  # í”¼í¬ ì‹œ 50ê°œê¹Œì§€
minAvailable: 5  # ìµœì†Œ 5ê°œ í•­ìƒ ê°€ìš©
```

**ê³„ì‚° ê·¼ê±°**:
- ëª©í‘œ RPS: 30,000
- Podë‹¹ ì²˜ë¦¬ëŸ‰: ~1,000 RPS (í”„ë¡ì‹œ ì—­í• )
- í•„ìš” Pod: 30,000 / 1,000 = 30
- ì—¬ìœ ë¶„ 50%: 30 * 1.5 = 45
- Max 50ìœ¼ë¡œ ì„¤ì •

#### Backend APIs (reservation, inventory, payment)
```yaml
minReplicas: 5   # í•­ìƒ 5ê°œ ìœ ì§€
maxReplicas: 30  # í”¼í¬ ì‹œ 30ê°œê¹Œì§€
minAvailable: 3  # ìµœì†Œ 3ê°œ í•­ìƒ ê°€ìš©
```

**ê³„ì‚° ê·¼ê±°**:
- ëª©í‘œ RPS: 10,000 (ë¶„ì‚° ê°€ì •)
- Podë‹¹ ì²˜ë¦¬ëŸ‰: ~500 RPS (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ + DB)
- í•„ìš” Pod: 10,000 / 500 = 20
- ì—¬ìœ ë¶„ 50%: 20 * 1.5 = 30

### 2. Redis ì•„í‚¤í…ì²˜

#### Master-Replica êµ¬ì„±
- **Master**: 1ê°œ (Write)
- **Replica**: 3ê°œ (Read)
- **ëª©ì **: Read ë¶€í•˜ ë¶„ì‚°

#### ì„¤ì • ìµœì í™”
```yaml
maxmemory: 512mb
maxmemory-policy: allkeys-lru  # LRU eviction
save: ""  # Persistence ë¹„í™œì„±í™” (ì„±ëŠ¥ ìš°ì„ )
appendonly: no  # AOF ë¹„í™œì„±í™”
```

### 3. ì• í”Œë¦¬ì¼€ì´ì…˜ ë ˆë²¨ ìµœì í™”

#### A. Connection Pooling
**Gateway API í™˜ê²½ë³€ìˆ˜ ì¶”ê°€**:
```yaml
- name: REDIS_POOL_SIZE
  value: "50"  # Connection pool size
- name: REDIS_POOL_TIMEOUT
  value: "3s"
- name: HTTP_MAX_IDLE_CONNS
  value: "100"
- name: HTTP_MAX_IDLE_CONNS_PER_HOST
  value: "50"
- name: HTTP_TIMEOUT
  value: "5s"
```

#### B. Circuit Breaker
```yaml
- name: CIRCUIT_BREAKER_THRESHOLD
  value: "5"  # 5ë²ˆ ì‹¤íŒ¨ í›„ ì°¨ë‹¨
- name: CIRCUIT_BREAKER_TIMEOUT
  value: "10s"
```

#### C. Rate Limiting (Per Pod)
```yaml
- name: RATE_LIMIT_RPS
  value: "1500"  # Podë‹¹ 1500 RPS ì œí•œ
- name: RATE_LIMIT_BURST
  value: "2000"
```

### 4. Kubernetes ë¦¬ì†ŒìŠ¤ ìµœì í™”

#### Gateway API Resources
```yaml
resources:
  requests:
    cpu: 500m      # 0.5 core
    memory: 512Mi
  limits:
    cpu: 2000m     # 2 cores max
    memory: 1Gi
```

**ê³„ì‚°**: 50 Pods * 0.5 CPU = 25 cores í•„ìš”

#### Backend API Resources
```yaml
resources:
  requests:
    cpu: 250m      # 0.25 core
    memory: 256Mi
  limits:
    cpu: 1000m     # 1 core max
    memory: 512Mi
```

**ê³„ì‚°**: 30 Pods * 3 APIs * 0.25 CPU = 22.5 cores í•„ìš”

### 5. Probe íŠœë‹

#### Readiness Probe (íŠ¸ë˜í”½ ìˆ˜ì‹  ì¤€ë¹„)
```yaml
readinessProbe:
  httpGet:
    path: /readyz
    port: 8000
  initialDelaySeconds: 5   # ë¹ ë¥¸ ì‹œì‘
  periodSeconds: 3         # ìì£¼ ì²´í¬
  timeoutSeconds: 2
  successThreshold: 1
  failureThreshold: 2      # 2ë²ˆ ì‹¤íŒ¨ì‹œ ì œì™¸
```

#### Liveness Probe (ì¬ì‹œì‘ íŒë‹¨)
```yaml
livenessProbe:
  httpGet:
    path: /healthz
    port: 8000
  initialDelaySeconds: 30  # ì¶©ë¶„í•œ ì‹œì‘ ì‹œê°„
  periodSeconds: 10        # ëœ ìì£¼ ì²´í¬
  timeoutSeconds: 5
  failureThreshold: 3      # 3ë²ˆ ì‹¤íŒ¨ì‹œ ì¬ì‹œì‘
```

### 6. Network ìµœì í™”

#### Service ì„¤ì •
```yaml
spec:
  type: NodePort
  sessionAffinity: ClientIP  # ì„¸ì…˜ ê³ ì •
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 300
```

#### Pod Anti-Affinity (ë¶„ì‚° ë°°ì¹˜)
```yaml
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - gateway-api
        topologyKey: kubernetes.io/hostname
```

### 7. ëª¨ë‹ˆí„°ë§ ì§€í‘œ

#### ì¶”ì í•´ì•¼ í•  ë©”íŠ¸ë¦­
1. **RPS**: ìš”ì²­ ì²˜ë¦¬ìœ¨
2. **Latency**: P50, P95, P99
3. **Error Rate**: 5xx ì—ëŸ¬ìœ¨
4. **CPU/Memory**: ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ 
5. **Pod Count**: í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ Pod ìˆ˜
6. **Redis**: Hit rate, command latency

#### Alert ê¸°ì¤€
```yaml
- RPS > 28,000: Warning (ì—¬ìœ  7%)
- RPS > 29,000: Critical (ì—¬ìœ  3%)
- P99 Latency > 100ms: Warning
- Error Rate > 1%: Critical
- CPU > 80%: Warning
```

### 8. Karpenter NodePool ì¡°ì •

#### Node Provisioning
```yaml
# on-demand-critical NodePool
spec:
  limits:
    cpu: "200"      # ìµœëŒ€ 200 cores
    memory: 400Gi
  disruption:
    consolidationPolicy: WhenUnderutilized
    consolidateAfter: 5m  # 5ë¶„ í›„ consolidation
```

### 9. ì˜ˆìƒ ë¹„ìš©

#### Compute Resources
- **Nodes**: ~15-20 m5.2xlarge (8 vCPU, 32GB)
- **ë¹„ìš©**: $0.384/hour * 20 = $7.68/hour = ~$5,600/month

#### Redis
- **ElastiCache ëŒ€ì•ˆ**: r6g.xlarge (4 vCPU, 32GB) = ~$250/month
- **í˜„ì¬ K8s**: ë¬´ë£Œ (ì´ë¯¸ ìˆëŠ” ë…¸ë“œ ì‚¬ìš©)

### 10. ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê³„íš

#### Phase 1: Warm-up
```bash
k6 run --vus 100 --duration 2m script.js
```

#### Phase 2: Ramp-up
```bash
k6 run --vus 500 --duration 5m --rps 10000 script.js
```

#### Phase 3: Peak Load
```bash
k6 run --vus 1000 --duration 10m --rps 30000 script.js
```

#### Phase 4: Stress Test
```bash
k6 run --vus 1500 --duration 5m --rps 40000 script.js
```

### 11. ì¥ì•  ëŒ€ì‘

#### Auto-recovery
- **HPA**: ìë™ ìŠ¤ì¼€ì¼ë§
- **Liveness Probe**: ìë™ ì¬ì‹œì‘
- **PDB**: Rolling update ì‹œ ê°€ìš©ì„± ë³´ì¥

#### Manual Intervention
```bash
# ê¸´ê¸‰ ìŠ¤ì¼€ì¼ ì—…
kubectl scale deployment gateway-api -n tacos-app --replicas=40

# Pod ê°•ì œ ì¬ì‹œì‘
kubectl rollout restart deployment gateway-api -n tacos-app
```

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Before Load Test
- [ ] HPA minReplicasë¥¼ ìµœì†Œ ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ ì„¤ì •
- [ ] Redis replica ë°°í¬
- [ ] Monitoring dashboard êµ¬ì„±
- [ ] Alert ì„¤ì •
- [ ] Node provisioning ì—¬ìœ  í™•ì¸

### During Load Test
- [ ] ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- [ ] CPU/Memory ì‚¬ìš©ë¥  ì¶”ì 
- [ ] Error rate í™•ì¸
- [ ] Latency ì¶”ì 

### After Load Test
- [ ] ê²°ê³¼ ë¶„ì„
- [ ] Bottleneck ì‹ë³„
- [ ] ìµœì í™” ì ìš©
- [ ] ì¬í…ŒìŠ¤íŠ¸

## ğŸ“ Best Practices

1. **ì ì§„ì  ì¦ê°€**: ê°‘ìê¸° 3ë§Œ RPSë¥¼ ì£¼ì§€ ë§ê³  ì ì§„ì ìœ¼ë¡œ
2. **Warm-up**: ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì¤€ë¹„ë  ì‹œê°„ ì œê³µ
3. **Connection Pre-warming**: ë¯¸ë¦¬ connection pool ì±„ìš°ê¸°
4. **Cache Pre-loading**: ìì£¼ ì‚¬ìš©í•˜ëŠ” ë°ì´í„° ì‚¬ì „ ë¡œë“œ
5. **Graceful Degradation**: ë¶€í•˜ê°€ ê³¼ë„í•˜ë©´ ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ
