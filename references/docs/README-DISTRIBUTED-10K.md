# K6 Distributed Load Test - 10k RPS

## ğŸ“Š ê°œìš”

ë‹¨ì¼ íŒŒë“œë¡œ 10k RPSë¥¼ ê°ë‹¹í•˜ê¸° ì–´ë ¤ì›Œ **3ê°œì˜ ë³‘ë ¬ Job**ìœ¼ë¡œ ë¶€í•˜ë¥¼ ë¶„ì‚°í•©ë‹ˆë‹¤.

### ë¶€í•˜ ë¶„ì‚° ì „ëµ
- **Part 1**: 3.3k RPS
- **Part 2**: 3.3k RPS  
- **Part 3**: 3.4k RPS
- **í•©ê³„**: 10k RPS

### ë‹¨ê³„ë³„ ë¶€í•˜ íŒ¨í„´
```
0-2ë¶„:  0 â†’ 1k RPS (Warm-up)
2-7ë¶„:  1k â†’ 3.3k/3.4k RPS (Ramp-up)
7-17ë¶„: 3.3k/3.4k RPS ìœ ì§€ (Peak load)
17-20ë¶„: 3.3k/3.4k â†’ 0 RPS (Ramp-down)
```

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1. ConfigMap ì ìš©
```bash
kubectl apply -f manifests/k6/job/k6-configmap-10k-distributed.yaml
```

### 2. Job ì‹¤í–‰ (ë³‘ë ¬ 3ê°œ)
```bash
kubectl apply -f manifests/k6/job/k6-job-parallel-10k.yaml
```

### 3. ìƒíƒœ í™•ì¸
```bash
# Job ìƒíƒœ
kubectl get jobs -n load-test -l test=10k-distributed

# Pod ìƒíƒœ
kubectl get pods -n load-test -l test=10k-distributed -o wide

# ì‹¤ì‹œê°„ ë¡œê·¸ (Part 1)
kubectl logs -n load-test -l part=1 -f

# ëª¨ë“  Part ë¡œê·¸ ë™ì‹œì— ë³´ê¸°
kubectl logs -n load-test -l test=10k-distributed --all-containers=true -f
```

### 4. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
```bash
# CPU/Memory ì‚¬ìš©ëŸ‰
kubectl top pods -n load-test -l test=10k-distributed

# ë…¸ë“œë³„ ë¦¬ì†ŒìŠ¤
kubectl get nodes -l workload-type=loadtest -o wide
```

## ğŸ“ˆ ê²°ê³¼ í™•ì¸

### Prometheus/Grafana
- Workspace: `ws-ec1155d6-1ea8-4822-b9e9-fdec9424dcb9`
- Metrics tags: `part=1`, `part=2`, `part=3`ìœ¼ë¡œ êµ¬ë¶„ ê°€ëŠ¥

### ì£¼ìš” ë©”íŠ¸ë¦­
```promql
# ì „ì²´ RPS
sum(rate(http_reqs_total[1m]))

# Partë³„ RPS
sum(rate(http_reqs_total{part="1"}[1m]))
sum(rate(http_reqs_total{part="2"}[1m]))
sum(rate(http_reqs_total{part="3"}[1m]))

# P95 Latency
histogram_quantile(0.95, sum(rate(http_req_duration_bucket[1m])) by (le))

# ì‹¤íŒ¨ìœ¨
rate(http_req_failed_total[1m]) / rate(http_reqs_total[1m])
```

## ğŸ§¹ ì •ë¦¬

### Job ì‚­ì œ
```bash
kubectl delete job -n load-test -l test=10k-distributed
```

### ConfigMap ì‚­ì œ
```bash
kubectl delete configmap -n load-test k6-script-10k-part1 k6-script-10k-part2 k6-script-10k-part3
```

### ì „ì²´ ì‚­ì œ
```bash
kubectl delete -f manifests/k6/job/k6-job-parallel-10k.yaml
kubectl delete -f manifests/k6/job/k6-configmap-10k-distributed.yaml
```

## âš™ï¸ ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­

### ê° Job Pod
- **CPU**: 1.5 cores (request), 2 cores (limit)
- **Memory**: 3Gi (request), 4Gi (limit)
- **maxVUs**: 1500 VUs per job

### ì „ì²´ (3 Jobs)
- **ì´ CPU**: 4.5 cores (request), 6 cores (limit)
- **ì´ Memory**: 9Gi (request), 12Gi (limit)
- **ì´ VUs**: ìµœëŒ€ 4500 VUs

### í•„ìš” ë…¸ë“œ
- **ìµœì†Œ**: t3a.large (2 core, 8GB) ë…¸ë“œ 3ê°œ
- **ê¶Œì¥**: loadtest nodepoolì— 3ê°œ ì´ìƒì˜ ë…¸ë“œ

## ğŸ”§ Troubleshooting

### Jobì´ Pending ìƒíƒœ
```bash
# ì´ë²¤íŠ¸ í™•ì¸
kubectl describe job k6-loadtest-10k-part1 -n load-test

# ë…¸ë“œ ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl get nodes -l workload-type=loadtest
kubectl top nodes -l workload-type=loadtest
```

### Podê°€ OOMKilled
```bash
# ë©”ëª¨ë¦¬ ì œí•œ ì¦ê°€ (Job YAML ìˆ˜ì •)
resources:
  limits:
    memory: "6Gi"  # 4Gi â†’ 6Gi
```

### RPSê°€ ëª©í‘œì— ëª» ë¯¸ì¹¨
```bash
# CPU ì œí•œ ì¦ê°€ ë˜ëŠ” Part 4 ì¶”ê°€
# maxVUs ì¦ê°€
```

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: K6 Operator Distributed Mode

í˜„ì¬ ë°©ì‹ì˜ í•œê³„:
- ìˆ˜ë™ Job ê´€ë¦¬
- ë¶€í•˜ ë¶„ì‚° ìˆ˜ë™ ê³„ì‚°
- ê²°ê³¼ ìˆ˜ì§‘ ë³µì¡

**K6 Operator ì‚¬ìš© ì‹œ**:
```yaml
apiVersion: k6.io/v1alpha1
kind: TestRun
metadata:
  name: k6-test-10k
spec:
  parallelism: 3       # ìë™ ë¶„ì‚°
  script:
    configMap:
      name: k6-script
  runner:
    resources:
      limits:
        cpu: 2
        memory: 4Gi
```

ìë™ìœ¼ë¡œ 3ê°œì˜ runnerê°€ ë¶€í•˜ë¥¼ ê· ë“± ë¶„ì‚°! ğŸš€
